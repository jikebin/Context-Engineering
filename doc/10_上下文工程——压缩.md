# 解构“压缩上下文”(Compress Context)

| 维度  | 上下文总结(Summarization) | 上下文裁剪(Trimming) |
| ----  | ------ | --------  |
| 核心机制  | LLM提炼(理解并重写) | 规则过滤(直接丢弃)  |
| 优点  | 智能，能保留核心语义<br/><br/>擅长处理非结构化长文本  | 快速，成本低<br/><br/>对保留部分信息保真度100%  |
| 缺点  | 慢，成本高(需要额外LLM调用)<br/><br/>有损压缩，可能丢失细节  | “笨拙”，可能粗暴丢弃重要信息<br/><br/>规则需人工精心设计  |
| 适用场景  | 处理巨型工具返回、总结长对话历史、Agent间工作交接。  | 管理常规对话历史(如滑动窗口)、清理已消化的旧信息。  |

## 上下文总结：智能的、有损的精炼

三大核心应用场景

### 总结对话历史

将多轮对话提炼为“任务概览、关键决策”等摘要，替换原文。

### 后处理工具反馈

在Agent接收前，先将海量的API返回结果(如网页全文)总结为核心要点。

### Agent间知识交接

子Agent向主Agent汇报工作时，提交浓缩后的“工作报告”而非完整过程。

### 核心挑战：信息保真度

“通过激进的压缩可能导致微妙但关键的上下文丢失。”
需要通过精心设计的压缩Prompt和**分层记忆系统**来管理风险。


## 上下文裁剪：基于规则的、无损的丢弃

### 方式一：硬编码启发式

核心策略：滑动窗口
- 设定固定规则，如：“永远只保留最近10轮对话”。当前对话产生时，最旧的一轮被自动丢弃。
- 优点：简单、快速、成本几乎为零。
- 缺点：“一刀切”，可能丢弃早期重要信息。

### 方式二：训练一个裁剪器

核心策略：智能过滤

- 训练一个轻量级分类模型，来判断上下文中的每一条信息是“应保留”还是“可丢弃”。
- 优点：比滑动窗口更智能，能学习保留关键信息。
- 缺点：需要额外模型，实现复杂。