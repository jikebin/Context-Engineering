# 《AI Agent的上下文工程：构建Manus的经验教训》

## 围绕KV缓存进行设计

生产级Agent的“北极星”指标：**KV缓存命中率**是优化Agent延迟(Latency) 与成本(Cost)的最重要单一指标。

### KV缓存的工作机制：前缀匹配（Prefix Matching）

第一次调用(缓存为空)
```sh
Prompt:[Prefix A] + [Content 1]
```
推理引擎完整计算[Prefix A],并将其Key/Value向量存入缓存。

第二次调用(利用缓存)
```sh
Prompt:[Prefix A] + [Content 2]
```
推理引擎检测到前缀匹配，直接加载缓存，跳过对[Prefix A]的重复计算，只对[Content2]进行增量计算。

结论：虽然每次都发送完整Prompt，但底层的推理引擎避免了大量重复计算。

### KV缓存的巨大价值：双重优化

#### 1.降低延迟(Latency)

通过复用缓存，极大缩短了LLM调用中最耗时的预填充(Prefill)阶段，让Agent的响应速度得到数量级提升。

#### 2.节省成本(Cost)

对于输入输出比高达100:1的Agent,输入Token的成本是大头。利用唤春可将这部分成本降低近10倍。
 
## 最大化缓存命中率的五大工程原则

### 1.开启与选型

自托管时，选用vLLM等现代推理框架，并确保KV缓存功能开启。

### 2.保证会话保持

分布式服务中，需要通过Session ID确保同一会话的请求被路由到同一个推理进程。

### 3.保持前缀稳定

任何字符或序列化顺序的改变，都会导致缓存从变更点开始失效。

### 4.上下文只追加

在上下文末尾追加是“缓存优化”的。修改或删除中间部分是“缓存杀手”。

### 5.明确标记缓存断点

“某些框架不支持自动增量前缀缓存，需手动插入断点...要考虑潜在的缓存过期问题，并至少确保断点包含系统提示的结尾。”

## 核型架构权衡：性能 VS 灵活性

### 性能优先(Manus)

“F1赛车“

* 策略：固定Prompt前缀，以最大化KV缓存命中率。
* 优点：极致的低延迟与低成本。
* 缺点：扩展性差，上下文臃肿。
* 适用：核心能力固定、对延迟极敏感的“专才”Agent.

### 灵活性优先(LangChain)

“全地形越野车”

* 策略：动态选择上下文，每次调用前缀都可能变化。
* 优点：扩展性强，上下文更精确。
* 缺点：牺牲KV缓存，延迟和成本更高。
* 适用：任务多样、需动态加载不同指令的“通才”Agent(例如客服识别意图后，加载“退货流程”指令集)。

